<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="Electromyography-Informed Facial Expression Reconstruction For Physiological-Based Synthesis and Analysis">
  <meta name="keywords" content="EIFER, 3DMM, Monocular 3D Face Reconstruction, Mimics and Muscles">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>EIFER: Electromyography-Informed Facial Expression Reconstruction For Physiological-Based Synthesis and Analysis</title>
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="./static/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/eifer.png" type="image/x-icon">

  <script src="./static/js/jquery-3.5.1.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>
  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item" href="https://timozen.github.io">
          <span class="icon">
            <i class="fas fa-home"></i>
          </span>
        </a>

        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link">
            More Links
          </a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://github.com/cvjena">
              Computer Vision Group Github
            </a>
            <a class="navbar-item" href="https://inf-cv.uni-jena.de">
              Computer Vision Group Homepage
            </a>
            <a class="navbar-item" href="https://www.uniklinikum-jena.de/hno/Klinik+f%C3%BCr+Hals__+Nasen_+und+Ohrenheilkunde-page--p-1.html">
              ENT Department - Jena University Hospital
            </a>
            <a class="navbar-item" href="https://www.uni-jena.de/en">
              Friedrich Schiller University Jena
            </a>
          </div>
        </div>
      </div>
    </div>
  </nav>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">

            <h1 class="title is-2 publication-title">
              <u>EIFER</u><br> Electromyography-Informed Facial Expression Reconstruction For Physiological-Based Synthesis and Analysis
            </h1>

            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://timozen.github.io">Tim BÃ¼chner</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=Iz6uugoAAAAJ">Christoph Anders</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=sbBPoc0AAAAJ">Orlando Guntinas-Lichius</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=bhpi3vgAAAAJ">Joachim Denzler</a><sup>1</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Computer Vision Group</span>
              <span class="author-block"><sup>2</sup>Jena University Hospital</span>
              <br>
              <span class="author-block">Friedrich Schiller University Jena, Germany</span>
            </div>
            <div class="is-size-4 publication-venue">
              <div style="transform: scaleX(-1); display: inline-block;">ðŸŽ‰</div>
              CVPR 2025
              ðŸŽ‰
            </div>
            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <!-- <span class="link-block">
                  <a href="#" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span> -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2503.09556" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv Preprint</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="./static/files/EIFER_supp.zip" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-file-archive"></i>
                    </span>
                    <span>Supplementary Material</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (Coming Soon)</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Models (Coming Soon)</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-images"></i>
                    </span>
                    <span>Data Sample(Coming Soon)</span>
                  </a>
                </span>
                <!-- Dataset Link. -->
                <span class="link-block">
                  <a href="" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-images"></i>
                    </span>
                    <span>Data Set(Coming Soon)</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
    </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <div class="content">
            <div class="publication-video">
              <iframe title="Teaser Video" src="./static/videos/TeaserWebsiteC.mp4" frameborder="0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen width="800"></iframe>
            </div>
            <!-- <img src="./static/images/teaser.webp" class="interpolation-image" alt="Rivers in East-Germany" width="auto" /> -->
            <p style="font-size: 0.95rem;" class="is-centered">
              <strong>Bridging the gap between mimics and muscles:</strong>
              Our method <strong>EIFER</strong> utilizes neural unpaired image-to-image translation to decouple facial geometry and appearance for muscle-activity-based expression synthesis and electrode-free facial electromyography.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            The relationship between muscle activity and resulting facial expressions is crucial for various fields, including psychology, medicine, and entertainment.
            The synchronous recording of facial mimicry and muscular activity via surface electromyography (sEMG) provides a unique window into these complex dynamics.
            Unfortunately, existing methods for facial analysis cannot handle electrode occlusion, rendering them ineffective.
            Even with occlusion-free reference images of the same person, variations in expression intensity and execution are unmatchable.
            Our electromyography-informed facial expression reconstruction (EIFER) approach is a novel method to restore faces under sEMG occlusion faithfully in an adversarial manner.
            We decouple facial geometry and visual appearance (e.g., skin texture, lighting, electrodes) by combining a 3D Morphable Model (3DMM) with neural unpaired image-to-image translation via reference recordings.
            Then, EIFER learns a bidirectional mapping between 3DMM expression parameters and muscle activity, establishing correspondence between the two domains.
            We validate the effectiveness of our approach through experiments on a dataset of synchronized sEMG recordings and facial mimicry, demonstrating faithful geometry and appearance reconstruction.
            Further, we synthesize expressions based on muscle activity and how observed expressions can predict dynamic muscle activity.
            Consequently, EIFER introduces a new paradigm for facial electromyography, which could be extended to other forms of multi-modal face recordings.
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero is-light is-small">
    <div class="hero-body">
      <div class="container">
        <h2 class="title is-3">Insights</h2>
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item">
            <img src="./static/images/eifer_qualitative.jpg" alt="Geometry Reconstruction Experiments" />
            <div class="caption">
              <strong>Facial Geometry Reconstruction</strong>
              <br>
              Our method faithfully reconstructs the facial geometry under strong sEMG electrode occlusion.
              It correctly captures the expression, identity, and pose of the subject.
            </div>
          </div>
          <div class="item">
            <img src="./static/images/eifer_qualitative_recon.jpg" alt="Appearance Reconstruction Experiments" />
            <div class="caption">
              <strong>Facial Appearance Reconstruction</strong>
              <br>
              Our method separates the facial appearance from the geometry, allowing for faithful reconstruction of the skin texture, lighting, and occluded electrodes.
              While existing methods inherently restore the occluded regions due to the utilized appearance model, our method creates more photorealistic results due to the adversarial cyclic training.
            </div>
          </div>
          <div class="item">
            <img src="./static/images/eifer_exp2emg.jpg" alt="Electrode-Free Facial EMG" />
            <div class="caption">
              <strong>Electrode-Free Facial EMG</strong>
              <br>
              Our method allows for the prediction of muscle activity based on observed facial expressions, this is a new paradigm for <strong>electrode-free facial electromyography</strong>.
              This enables the reconstruction of muscle activity without the need for electrodes, providing a new way to analyze facial expressions.
              We demonstrate that our method can predict muscle activity from observed expressions and synthesize expressions based on muscle activity.
              We provide next to <strong>EIFER</strong> several <strong>Exp2EMG</strong> models for existing extractors like <strong>DECA</strong>, <strong>EMOCAv2</strong>, <strong>SMIRK</strong>,<strong>Deep3DFace</strong>, and <strong>FOCUS</strong>.
              Hence, both <strong>FLAME</strong> and <strong>BFM</strong> expression spaces are connected to the muscle activity.
            </div>
          </div>
          <div class="item">
            <img src="./static/images/eifer_emg2exp.jpg" alt="Expression Synthesis" />
            <div class="caption">
              <strong>Expression Synthesis</strong>
              <br>
              In the second phase of <strong>EIFER</strong> we learn to synthesize facial expressions based on muscle activity.
              This allows to generate expression based solely on muscle activity and provides a new way to analyze facial expressions, especially interesting for possible advances in camera-free animation captures.
              While <strong>EIFER</strong> creates the most realistic results, we will also publish EMG2Exp of <strong>MC-CycleGAN+Model</strong> combinations.
              Therefore, both the expression space of both <strong>FLAME</strong> and <strong>BFM</strong> are not connected to the muscle activity.
            </div>
          </div>
          <div class="item">
            <img src="./static/images/01_face-at-rest_Geo.jpg" alt="Expression Synthesis" />
            <div class="caption">
              <strong>More Geometry Examples - Fact At Rest</strong>
            </div>
          </div>
          <div class="item">
            <img src="./static/images/02_eye-tight_Geo.jpg" alt="Expression Synthesis" />
            <div class="caption">
              <strong>More Geometry Examples - Eyes closed tightly</strong>
            </div>
          </div>
          <div class="item">
            <img src="./static/images/03_smile-open_Geo.jpg" alt="Expression Synthesis" />
            <div class="caption">
              <strong>More Geometry Examples - Smile Open</strong>
            </div>
          </div>
          <div class="item">
            <img src="./static/images/04_snarl_Geo.jpg" alt="Expression Synthesis" />
            <div class="caption">
              <strong>More Geometry Examples - Snarl</strong>
            </div>
          </div>
          <div class="item">
            <img src="./static/images/05_nose_Geo.jpg" alt="Expression Synthesis" />
            <div class="caption">
              <strong>More Geometry Examples - Nose wrinkling</strong>
            </div>
          </div>

        </div>
      </div>
    </div>
  </section>


  <!-- <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <h2 class="title is-3">Related Links</h2>
            <div class="content has-text-justified">
              <p>
                There's a lot of excellent work that was introduced around the same time as ours.
              </p>
              <p>
                <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an
                idea similar to our windowed position encoding for coarse-to-fine optimization.
              </p>
              <p>
                <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
                both use deformation fields to model non-rigid scenes.
              </p>
              <p>
                Some works model videos with a NeRF by directly modulating the density, such as <a href="https://video-nerf.github.io/">Video-NeRF</a>, <a href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a href="https://neural-3d-video.github.io/">DyNeRF</a>
              </p>
              <p>
                There are probably many more by the time you are reading this. Check out <a href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.
              </p>
            </div>
          </div>
        </div>
      </div>
  </section>-->

  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX (arXiv Preprint)</h2>
      <span>
        If you find our work useful, work with our models, utilize our data set for your research, or use our code, please cite our work:
      </span>
      <pre><code>@article{buchner2025electromyography,
      title={Electromyography-Informed Facial Expression Reconstruction for Physiological-Based Synthesis and Analysis},
      author={B{\"u}chner, Tim and Anders, Christoph and Guntinas-Lichius, Orlando and Denzler, Joachim},
      journal={arXiv preprint arXiv:2503.09556},
      year={2025}
}
</code></pre>
    </div>
  </section>

  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <a class="icon-link" href="./static/files/EIFER.pdf">
          <i class="fas fa-file-pdf"></i>
        </a>
        <a class="icon-link" href="https://timozen.github.io" class="external-link" disabled>
          <i class="fab fa-github"></i>
        </a>
      </div>
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
              The original design was borrowed from <a href="https://github.com/nerfies/nerfies.github.io">here</a>.
              If you want to use the design, please follow the original license and link.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>